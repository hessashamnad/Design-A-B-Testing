---
title: "P7 Design A/B test"
author: "Hessa Shamnad"
date: "March 28, 2016"
output: html_document
---


##Experiment Design
###Metric Choice
_List which metrics you will use as invariant metrics and evaluation metrics here._

* **Invariant metrics:** Number of cookies, Number of clicks, Click-through-probability
* **Evaluation metrics:** Gross conversion, Retention, Net conversion

_For each metric, explain both why you did or did not use it as an invariant metric and why you did or did not use it as an evaluation metric. Also, state what results you will look for in your evaluation metrics in order to launch the experiment._

* **Number of cookies:** Since unit of diversion is cookies, so this should be invariant between experiment group and control group.
* **Number of user-ids:** This may decrease in experiment group, and number is not a good evaluation metrics because it is not normalized. So number of user-ids is neither invariant metrics nor good evaluation metrics.
* **Number of clicks:** Since cookies are randomly assigned to two groups, and clicks happen before our experiment, so this should be invariant.
* **Click-through-probability:** Since number of cookies and number of clicks are invariant, click-through-probability equals to number of clicks divided by number of cookies, so this is invariant as well.
* **Gross conversion:** : A good evaluation metrics since it's directly dependent on the effect of the experiment & allow us to show whether we managed to decrease the cost of enrolment aren't likely to be real customers. In detail, in the experiment group the user can make a decision, if he has enough time to devote for course, he will be enrolled or if not, he will continue with the free courseware without enrolling. Vice versa, for the control group, no pop-up would be displayed regardless of the time availability enrolls for the course. The underlying assumption would be the gross conversion in the control group is higher than experiment group Therefore, it can be used as an evaluation metric to check if the experiement makes a significant difference in the enrolment. 
* **Retention:** As mentioned in gross conversion metric, in the experiment group, users are completely aware of the time commitment, they made a deliberate decision to enrol the course or continue with the free courseware option. We expect to see the following result:  The rentention might be high for the experiment group, since fewer users enrolled based on their time commitment & likely to to remain enrolled after 14-day period and start their first initial payment. But for the control group, many users enroll for the courseware, since not many of them have the required time, the cancellation rate might be high and hence the retention rate will be low.
* **Net conversion:** This may decrease in experiment group since some people would click "Start free trial" button and has less than 5 hours each week are suggested not to enroll, thus would decrease people who enroll, thus may decrease number of people pay. Net conversion is about probability to succeed, so this is good evaluation metrics.

I will look at **Gross conversion** and **Net conversion**. The first metric will show us whether we lower our costs by introducing the screener. The second metric will show how the change affects our revenues.

To launch the experiment, I will require **Gross conversion** to have a practically significant decrease, and **Net conversion** to have a statistically significant increase.

###Measuring Standard Deviation
_List the standard deviation of each of your evaluation metrics._
Calculation ([Data](https://docs.google.com/spreadsheets/d/1MYNUtC47Pg8hdoCjOXaHqF-thheGpUshrFA21BAJnNc/edit#gid=0)): 
```
+ Gross conversion: se = sqrt(0.20625*(1-0.20625)/3200) = 0.00715 (correspond to 3200 clicks & 40000 pageviews).
For 50000 pageviews, we have new_se = 0.00715 * sqrt(40000/5000) = 0.0202 
+ Retention: se = sqrt((0.53*(1-0.53)/660) * sqrt(40000/5000))) = 0.549
+ Net conversion: se = sqrt(0.1093125*(1-0.1093125)/3200) = 0.0055159 (correspond to 3200 clicks & 40000 pageviews).
For 50000 pageviews, we have new_se = 0.00715 * sqrt(40000/5000) = 0.0156 
```

Final Results:
```
Gross conversion: 0.0202
Retention: 0.0549
Net conversion: 0.0156
```
_For each of your evaluation metrics, indicate whether you think the analytic estimate would be comparable to the the empirical variability, or whether you expect them to be different (in which case it might be worth doing an empirical estimate if there is time). Briefly give your reasoning in each case_

Both Gross Conversion and Net Conversion using number of cookies as denominator, which is also unit of diversion. Here, the unit of diversion is equal to unit of analysis, which indicate the analytical estimate would be comparable to the emperical variability. 

For Retention, the denominator is "Number of users enrolled the courseware" which is not similar as Unit of Diversion. The unit of analysis and the unit of diversion are not the same therefore the analytical an the empirical estimates are different.


###Sizing
####Number of Samples vs. Power
_Indicate whether you will use the Bonferroni correction during your analysis phase, and give the number of pageviews you will need to power you experiment appropriately_

No, I did not use Bonferronicorrection during my analysis phase. The metrics in the test has high correlation (covariant) and the Bonferroni correction will be too conservative to it.


After much consideration from back-of-the-envelope calculation, I realised the amount of pageview for retention as evaluation metrics would need almost half-a-year for testing even if we direct 50% of traffic to that experiment, which is completely not a economic feasible timeline for a A/B Testing result. Therefore, I have iterate my evaluation metrics and use Gross Conversion and Net Conversion as evaluation metrics. Using [Evan Miller](http://www.evanmiller.org/ab-testing/sample-size.html), the result can be referred below:

```
Probability of enrolling, given click:
20.625% base conversion rate, 1% min d.
Samples needed: 25,835

Probability of payment, given click:
10.93125% base conversion rate, 0.75% min d.
Samples needed: 27,413 (chosen)

Therefore, pageview/group = 27413/0.08 = 342662.5
Total pageview = 342662.5*2 = 685325
*Note1 : only 0.08 pageview leads to click.
*Note2: double pageview because we need total pageview for both experiment & control group
```
####Duration vs. Exposure
_Indicate what fraction of traffic you would divert to this experiment and, given this, how many days you would need to run the experiment._

I would divert 70% of the traffic to the experiment. Given that, the experiment most probably will take 25 days, which is a reasonable time for our needs.

_Give your reasoning for the fraction you chose to divert. How risky do you think this experiment would be for Udacity?_

The experiment is not extremely risky given that it does not affect existing paying customers, and is simple enough that there is a low chance of bugs occurring in the process. Nevertheless it may have a substantial impact on new enrollments, and diverting 100% of the traffic may thus not be advisable.

##Experiment Analysis
###Sanity Checks
_For each of your invariant metrics, give the 95% confidence interval for the value you expect to observe, the actual observed value, and whether the metric passes your sanity check._

```
Number of cookies: [.4988, .5012]; observed .5006; PASS
Number of clicks on "Start free trial": [.4959, .5041]; observed .5005; PASS
Click-through-probability on "Start free trial": [.0812, .0830]; observed .0822; PASS
```

###Result Analysis
####Effect Size Tests
_For each of your evaluation metrics, give a 95% confidence interval around the difference between the experiment and control groups. Indicate whether each metric is statistically and practically significant._

```
Gross conversion: [-.0291, -.0120], statistically significant, practically significant
Net conversion: [-.0116, .0019], not statistically significant, not practically significant
```

####Sign Tests
_For each of your evaluation metrics, do a sign test using the day-by-day data, and report the p-value of the sign test and whether the result is statistically significant._

```
Gross conversion: .0026, statistically significant
Net conversion: .6776, not statistically significant
```

####Summary
_State whether you used the Bonferroni correction, and explain why or why not. If there are any discrepancies between the effect size hypothesis tests and the sign tests, describe the discrepancy and why you think it arose._

I did not use a Bonferroni correction because we are only testing one variation. It might be useful to apply the Bonferroni correction if we decide to do post-test segmentation on the results, for example based on browser type or countries of origin.

###Recommendation
_Make a recommendation and briefly describe your reasoning._

My final recommendation is we should *NOT* launch the experiment. The metrics I was interested in were Gross conversion and Net conversion.

Gross conversion turned out to be negative and practically significant. This is a good outcome because we lower our costs by discouraging trial signups that are unlikely to convert. Net conversion unfortunately ended up being statistically and practically insignificant and the confidence interval includes negative numbers. Therefore, there is a risk that the introduction of the trial screener may lead to a decrease in revenue.

We should therefore consider test other designs of the screener before we decide whether to release the feature, or abandon the idea entirely.

##Follow-Up Experiment
_Give a high-level description of the follow up experiment you would run, what your hypothesis would be, what metrics you would want to measure, what your unit of diversion would be, and your reasoning for these choices._

As a Udacity student, I was very irritated the fact i was not able to meet the prerequisite 1 year ago, when I was having no knowledge of Python all. I ended up paying Udacity for 1 month without understand any course materials and not even able to understand the later project. This would leads me with so much frustration and decided to quit the program half-heartedly, concluded I was not meant to learn data analysis. Luckily, I found other resources and it helped me learned the foundation for the program and now I'm almost completeing the whole nanodegree. So i thought , why not do something that will make students feel more comfortable by getting the basics, so that no one should feel inadequate reagrding the course. this will help students to learn more or freshen up what they have learnt.Thus, I proposed Udacity to launch a new initiative to help those students in need, which is: __A new pre-course required students to complete the project within that 14 days trial__. The course will cover basic knowledge of Python & Data Analysis which would be doable if the students spent minimum 10 hours/week. Through this pre-course, students know the level of fluency the nanodegree required and also act as an encouragement that if they willing to learn, course completion is a possible task. 

__Null hypothesis__ by creating such pre-course, it will not increase Retention by a practically significant amount.

The pre-course will be randomly assigned to a Control & Experiment Group. The whole courses for Control Group would not change and remained the same while courses for Experiment Group will have this one pre-course (which required them to finish within 14 days)

__Unit of diversion__ is  User-IDs, users once enrolled in this new pre-course will be tracked by user-id and this change only impacts what happens after a free trial account is created. The main purpose is we want users to only see this "new offering" one version at a time, and not to confuse user by showing diffrent version option at different time.

__Invariant metric__ is number of cookies since unit of diversion is cookie, and because users are not exposed to this new course after hitting "Free Trials" screener button. 

__Evaluation metric__ is Net Conversion, whether rendering a "new pre-course"  helps increase the ratio of users who make payment over those who decided only to try the program.  If the CI shows practically and statistically significant, this will prove that this initiative will increase the eventual revenue by having more people signing up for the program. 

If Net Conversion is positive and practically significant at the end of the experiment, we can roll out this new course to whole Nanodegree program and added the pre-course as the official course of Data Analyst nanodegree program.

##Resources
* [Nine Common A/B Testing Pitfalls and How to Avoid Them](http://adobe-target.com/content/dam/adobe/Common_AB_TestingPitfalls_2014.pdf)